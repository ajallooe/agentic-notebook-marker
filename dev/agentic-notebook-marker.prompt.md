# Agentic Marker System Design

I have a project which I want to accomplish ASAP. This task is helping me create a system for draft marking for assignments submitted by students in a class I teach using LLMs invoked fromCLI. The students submit Jupyter notebook files as their submissions. 
There are two kinds of assignments:

## 1. Fill-in-the-blank style assignments

These Jupyter notebook files are filled copies of a fill-in-the-blank kind of notebook, where students start with a starter notebook, and they have to implement what is asked. Some assignments have rubrics, which are included in the notebook, and some don't. Also, the starter notebooks are structured: activities that have to be done by students are specified by "[A<i>]" in which i is the activity number. The sections students have to fill are enclosed between text cells "*Start student input* ↓" and "*End student input ↑*". There may be multiple cells between those, and both code and text kinds. They are free to add more cells if they like; however, they are asked not to change anything outside those limits. Also, some cells come with some pieces of code already filled in. I've broken down the task into multiple steps:

1. The first step is for a "marking pattern designer" agent to go through the unfilled version of the notebook. The agent identifies the activities that have to be done, how many there are, and their order, and writes a concise but detailed account of what needs to be done for each activity in a separate markdown file. The agent also writes a concise summary of what students were supposed to have accomplished before, especially as it pertains to the activity at hand. It also sets up criteria for what successful completion looks like. This all goes into those per-activity markdown files. The goal is for these markdown files to be used by another "marker" agent, so the markdown files should be written with that in mind. Finally, this agent writes an overall markdown file which includes the rubric, if already included, or creates a sensible rubric if there is none. If creating a sensible rubric requires knowledge about student proficiency levels and is not inferable from the base notebook itself, the agent may ask the operator, who is the instructor of the course, to provide guidance, preferably by asking the agent specific questions about whether the agent can assume the students have a specific qualification or not, but if other forms can help better, it is okay. Asking the instructor to describe the proficiency of the students is an available but last resort option. This questioning, if necessary, is done after the agent has gone through the notebook and written a detailed internal guidance document in markdown for itself (also, at the beginning, the instructor may also optionally provide and assignment description package which explain the goals of the assignment, which can also be used, if provided). Then, upon clarification, the agent first creates a rubric file if it does not exist and displays it, signalling readiness to move on to the next stage. The instructor can change the rubric until they are happy. Upon confirmation, the agent then creates those per activity marking design documents. Finally, they create that final overall document, which includes the rubric, given or created, unchanged or updated, extended by the detailed marking criteria and expectations it created per activity.
2. Then, if there an n students in the class, and m activities to be done, m sets of n "marker" agents are called, one set per activity. Each agent is provided with the previously created document with the marking pattern design for that activity. These agents are supposed to mark the solutions not with a concrete numerical scheme, but rather identify all the mistakes the students made, as well as their positive points, and record the result of their marking in a markdown document named after the activity number, as well as the student's name.
3. Then a "normalizer" agent is launched, one per activity, which looks at all the markings done in those markdown files created by the marker agents, for all students on that activity and based on that, unifies them into a table of mistakes and positives and identify the severity of mistakes on a scale from 1 to 10 and the level of positives, again on a scale from 1 to 10.  They also suggest a number for marks to be lost for mistakes, and possible bonus points given for positives and write them to another markdown file for that activity the agent was handling.
4. This is then aggregated in one document for all activities and presented to the instructor in a table, which has alongside it a distribution of marks students will get if the marking scheme is used. The instructor can change the marking number and observe the results on the distribution of the marks. This interface can be a Google Sheets or Excel file or any form that makes the best sense for this (it needs to have the changeable table and the live updating mark distribution graph). This adjustment dashboard shows numerical statistics as well as a histogram of the marks distribution.
5. Upon the instructor finalizing and approving the final marking scheme, n "unifier" agents are run, one per student, which assign the marks based on that scheme and also review the student submission in its entirety to see if there are broader patterns that warrant rare changes for a specific student based on patterns in the overall notebook. These agents, however, do not change anything on their own but rather present their suggestions to the instructor and let him decide what to do. Also, these agents guess how likely it is for that student to have used an LLM to solve the assignment rather than doing it themselves, as well as any signs that they accomplished something but did not know what they were doing or any other sign related to cheating. They consider these factors when making suggestions to the instructor. Finally at the end, they will produce a concise feedback card for each student listing the marks they got for each activity, what they lost marks on, as well as their total and a brief feedback on what they did.
6. Finally, an "aggregator" agent runs through all these per-student documents and puts all of that in a CSV format, which has the student's name, total mark, feedback card, and marks for each activity in that exact order in a format appropriate for that. There may optionally a base CSV file provided by the instructor (for example one coming from Moodle gradebook export) which has student names, in which case, it can be used to create that final document matching the format.

## 2. Free-form assignments

The workflow here is similar to the above, but instead of starting from a fill-in-the-blank notebook, students start from an empty notebook and have to do everything from scratch. The marking pattern designer agent has to figure out what the activities are based on the assignment description provided by the instructor in a separate markdown file, as well as any rubric provided. Specifically, the steps are:

1. The first step is for a "marking pattern designer" agent to go through the assignment description package (set of files). The agent identifies the activities that have to be done and writes a concise but detailed account of what needs to be done. It also sets up criteria for what successful completion looks like. This all goes into a markdown files. The goal is for these markdown files to be used by the "marker" agent, so the markdown files should be written with that in mind. This file should also includes the rubric, if already included, or create a sensible rubric if there is none. If creating a sensible rubric requires knowledge about student proficiency levels and is not inferable from the base notebook itself, the agent may ask the operator, who is the instructor of the course, to provide guidance, preferably by asking the agent specific questions about whether the agent can assume the students have a specific qualification or not, but if other forms can help better, it is okay. Asking the instructor to describe the proficiency of the students is an available but last resort option. This questioning, if necessary, is done after the agent has gone through the notebook and written a detailed internal guidance document in markdown for itself. Then, upon clarification, the agent first creates a rubric file if it does not exist and displays it, signalling readiness to move on to the next stage. The instructor can change the rubric until they are happy. Upon confirmation, the agent creates a final overall document.
2. Then, if there an n students in the class, n "marker" agents are called, one set per student. Each agent is provided with the previously created document with the marking pattern design. These agents are supposed to mark the solutions not with a concrete numerical scheme, but rather identify all the mistakes the students made, as well as their positive points, and record the result of their marking in a markdown document named after the activity number, as well as the student's name.
3. Then a "normalizer" agent is launched which looks at all the markings done in those markdown files created by the marker agents, for all students, and based on that, unifies them into a table of mistakes and positives and identify the severity of mistakes on a scale from 1 to 10 and the level of positives, again on a scale from 1 to 10.  They also suggest a number for marks to be lost for mistakes, and possible bonus points given for positives and write them to another markdown file.
4. This is then presented to the instructor in a table, which has alongside it a distribution of marks students will get if the marking scheme is used. The instructor can change the marking number and observe the results on the distribution of the marks. This interface can be a Google Sheets or Excel file or any form that makes the best sense for this (it needs to have the changeable table and the live updating mark distribution graph). This adjustment dashboard shows numerical statistics as well as a histogram of the marks distribution.
5. Upon the instructor finalizing and approving the final marking scheme, n "unifier" agents are run, one per student, which assign the marks based on that scheme and also review the student submission to see if there are broader patterns that warrant rare changes for a specific student based on patterns in the overall notebook. These agents, however, do not change anything on their own but rather present their suggestions to the instructor and let him decide what to do. Also, these agents guess how likely it is for that student to have used an LLM to solve the assignment rather than doing it themselves, as well as any signs that they accomplished something but did not know what they were doing or any other sign related to cheating. They consider these factors when making suggestions to the instructor. Finally at the end, they will produce a concise feedback card for each student listing the marks they got, what they lost marks on, as well as their total and a brief feedback on what they did.
6. Finally, an "aggregator" agent runs through all these per-student documents and puts all of that in a CSV format, which has the student's name, total mark, feedback card, and marks for each activity in that exact order in a format appropriate for that. There may optionally a base CSV file provided by the instructor (for example one coming from Moodle gradebook export) which has student names, in which case, it can be used to create that final document matching the format.

## Implementation details

I am trying to accomplish this with CLI LLM tools, specifically Claude Code, Codex and Gemini (search the web for information and documentation if you don't know how these work). Not all these tools have the same usage details, so first we have to write a unified caller, which is a shell file that takes as input a prompt, a run mode which is either interactive (default) or headless, an optional provider, i.e., "claude", "gemini", "codex" or an optional model "claude-sonnet-4", "gemini-3-pro-preview", "gpt-5.1" (the provider can be from model name), ..., which the utilities and agents can use to call other agents. Calls to "marking pattern designer" and to "aggregator" agents are done in interactive sessions (for Claude you can supply the initial prompt via stdin if there isn't a way to start an interactive session but one where you provide the initial prompt via CLI, or a better way if you know one) and the conversation transcript is saved to a file via:
  
  ```sh
  script <appropriate_session_name>
  <call>
  # ...interact...
  exit. 
  ```
  
or a better way if you know one. These agents will indicate to the user when they can exit for the rest of the marking process to continue when their part is done. Calls to "marker", "normalizer" and "unifier" agents are done using headless or exec mode and the output of every call is captured in an appropriate file, so we keep track of the entire event. If not specified per agent type, all agents will use the default provider and model specified in a project overview file.

You can assume the assignments are located within a subdirectory in the project directory called assignments, which has a structure like this:

```files
.
├── assignments
│   ├── <assignment_name>
│   │   ├── <base_file>.ipynb
│   │   ├── <other_file_1>.ipynb\
│   │   ├── ...
│   │   ├── overview.md
│   |   └── submissions
|   │       ├── <section_name>
|   │       │    ├── subdir_2
|   │       |        ...
|   │       │        ├── subdir_k
|   │       |        │    ├── <submission_name>.ipynb
|   │       |        │    ├── ...
|   │       │        └── ...
|   │       └── ...
```
Note that submission notebook files may be in a nested subdirectory structure within each section. Also, that they may contain spaces in their names, so take that into consideration when handling them.
Put all your artefacts under a `assignments/<assignment_name>/processed` subdirectory, in appropriate subdirectories and with appropriate names. The file `overview.md` is where we will keep the optional project overview with links to other files in the description package if there are more, including default provider and model and the name of the base file in the structured case, as well as any other relevant information. You can set up a specific structure for how to specify the including default provider and model and the name of the base file, but it does not necessarily have to be the first lines in the file.

Also, I would like the instructor to be informed of the marking progress. a way to have a progress indicator, which shows we are at what question and which of the students have been already marked alongside a progress percentage, all printed like a console log, that would be a nice extra. We progress activity by activity and inside that student by student in the structured case and student by student in the free-form case. We inform what is being done and when it is completed and we also display <current_activity_index>/<total_activities_number> and <current_student_index>/<total_students_number> as well as in percentages.

Also, to minimize context, in the structured assignment case, we can break the submission of each student down to exactly what they did for each activity and only feed that to the marker agents via writing an activity extractor function we develop. In the structured case, all students are allowed to change is sandwiched between text cells "Start student input ↓" and "End student input ↑". Under a specific [A<i>] indicating the assignment. There may be multiple of these delimited input places per activity, all of which are distinct and belong to that activity until the next activity, i.e., [A<i+1>] in a text cell further down. Also, there may be more than one cell in each delimited input places and it can be any combo of text and code cells. So, we can extract all these cells sandwiched between those delimited cells and all under a specific activity, all for that activity. You can find an example of a base notebook of the structured kind in the `dev/examples/structured_assignment_example.ipynb` file, if that helps you.

Also, be mindful of the prompts you create for the agents if there is a maximum token size for them. 

Acknowledge you understand what I am trying to do, present you with suggestions for the instructor mark adjustment document kind (with the editable tables and live distribution graph) as well as the final output aggregated marks document, then list all the artefacts and documents that are produced in this process, and lastly suggest a plan on how to implement this project using Claude Code Web. Also, comment on what you think about this structure I devised for this project. Let me approve and verify this plan, and then let's start building it ASAP.
